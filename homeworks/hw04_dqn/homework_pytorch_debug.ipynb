{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqZ2EwnTZdC8"
      },
      "source": [
        "# Deep Q-Network implementation.\n",
        "\n",
        "This homework shamelessly demands you to implement DQN — an approximate Q-learning algorithm with experience replay and target networks — and see if it works any better this way.\n",
        "\n",
        "Original paper:\n",
        "https://arxiv.org/pdf/1312.5602.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv7XJfXaZdC9"
      },
      "source": [
        "**This notebook is given for debug.** The main task is in the other notebook (**homework_pytorch_main**). The tasks are similar and share most of the code. The main difference is in environments. In main notebook it can take some 2 hours for the agent to start improving so it seems reasonable to launch the algorithm on a simpler env first. Here it is CartPole and it will train in several minutes.\n",
        "\n",
        "**We suggest the following pipeline:** First implement debug notebook then implement the main one.\n",
        "\n",
        "**About evaluation:** All points are given for the main notebook with one exception: if agent fails to beat the threshold in main notebook you can still get points for beating the threshold in debug notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ioIEVODJZdC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a39882-b22f-4d1f-d932-a8bd649f6c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules and not os.path.exists(\".setup_complete\"):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/atari_wrappers.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/utils.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/replay_buffer.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/framebuffer.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ[\"DISPLAY\"] = \":1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u8OFQOtGojc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c2921e-cdcd-47ea-ba9a-264e5109f798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDZqlI3kZdC9"
      },
      "source": [
        "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for PyTorch, but you find it easy to adapt it to almost any Python-based deep learning framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dsYq558wZdC-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6ypPZ8e6ZdC-"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j8EGNlSZdC-"
      },
      "source": [
        "### CartPole again\n",
        "\n",
        "Another env can be used without any modification of the code. State space should be a single vector, actions should be discrete.\n",
        "\n",
        "CartPole is the simplest one. It should take several minutes to solve it.\n",
        "\n",
        "For LunarLander it can take 1-2 hours to get 200 points (a good score) on Colab and training progress does not look informative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v-5u-CcQZdC-"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "\n",
        "def make_env():\n",
        "    # some envs are wrapped with a time limit wrapper by default\n",
        "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\").unwrapped\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AmFXRrkqZdC-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "25911879-1b60-4805-a01c-e4e7bdc0ac94"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = make_env()\n",
        "env.reset()\n",
        "plt.imshow(env.render())\n",
        "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOyWgOmvZdC-"
      },
      "source": [
        "### Building a network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqpThLZXZdC-"
      },
      "source": [
        "We now need to build a neural network that can map observations to state q-values.\n",
        "The model does not have to be huge yet. 1-2 hidden layers with < 200 neurons and ReLU activation will probably be enough. Batch normalization and dropout can spoil everything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UVlpkvZOZdC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30fecb7-1a8d-4219-e409-fcf2b4ac4557"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# those who have a GPU but feel unfair to use it can uncomment:\n",
        "# device = torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "RFva1cpyZdC-"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.distributions.normal import Normal\n",
        "import re\n",
        "\n",
        "\n",
        "class DQNAgent(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "        self.state_shape = state_shape\n",
        "        # Define your network body here. Please make sure agent is fully contained here\n",
        "\n",
        "        print(state_shape[0])\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(state_shape[0], 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, self.n_actions)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, state_t):\n",
        "        \"\"\"\n",
        "        takes agent's observation (tensor), returns qvalues (tensor)\n",
        "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
        "        \"\"\"\n",
        "        # Use your network to compute qvalues for given state\n",
        "        qvalues = self.output(state_t)\n",
        "\n",
        "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
        "        assert len(\n",
        "            qvalues.shape) == 2 and qvalues.shape[0] == state_t.shape[0] and qvalues.shape[1] == n_actions\n",
        "\n",
        "        return qvalues\n",
        "\n",
        "    def get_qvalues(self, states):\n",
        "        \"\"\"\n",
        "        like forward, but works on numpy arrays, not tensors\n",
        "        \"\"\"\n",
        "        model_device = next(self.parameters()).device\n",
        "        states = torch.tensor(states, device=model_device, dtype=torch.float)\n",
        "        qvalues = self.forward(states)\n",
        "        return qvalues.data.cpu().numpy()\n",
        "\n",
        "    def sample_actions(self, qvalues):\n",
        "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "\n",
        "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "        best_actions = qvalues.argmax(axis=-1)\n",
        "\n",
        "        should_explore = np.random.choice(\n",
        "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
        "        return np.where(should_explore, random_actions, best_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Bv1s5JKzZdC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f76db93d-b388-42a1-94c4-5d8cd1a59745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vazC0DPQZdC_"
      },
      "source": [
        "Now let's try out our agent to see if it raises any errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "e-Sg1cqPZdC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f7cc04-4c0c-4886-a628-b2debeb4360b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(11.0)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000, seed=None):\n",
        "    \"\"\"Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward.\"\"\"\n",
        "    rewards = []\n",
        "    for _ in range(n_games):\n",
        "        s, _ = env.reset(seed=seed)\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            qvalues = agent.get_qvalues([s])\n",
        "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
        "\n",
        "            s, r, terminated, truncated, _ = env.step(action)\n",
        "            reward += r\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)\n",
        "\n",
        "evaluate(env, agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0NzjUEZdC_"
      },
      "source": [
        "### Experience replay\n",
        "For this assignment, we provide you with experience replay buffer. If you implemented experience replay buffer in last week's assignment, you can copy-paste it here in main notebook **to get 2 bonus points**.\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHyCO4TuZdC_"
      },
      "source": [
        "#### The interface is fairly simple:\n",
        "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
        "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
        "* `len(exp_replay)` - returns number of elements stored in replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.__version__"
      ],
      "metadata": {
        "id": "7InMe7rDTBdc",
        "outputId": "dbbd7973-1873-4b26-a6a7-88df52cbbea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "wQEHwR1AZdC_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "662456b2-5552-4ab5-94ba-dfe6ccd5210c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-3983d0c8634e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mobs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_replay\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"experience replay size should be 10 because that's what maximum capacity is\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/replay_buffer.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mdone_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexecuting\u001b[0m \u001b[0mact_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mresulted\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mend\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0midxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/replay_buffer.py\u001b[0m in \u001b[0;36m_encode_sample\u001b[0;34m(self, idxes)\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0;31m#data = self._storage[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m           \u001b[0;31m#obs_t, action, reward, obs_tp1, done = data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m           \u001b[0;31m#obses_t.append(np.array(obs_t))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m           \u001b[0;31m#actions.append(np.array(action))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword."
          ]
        }
      ],
      "source": [
        "from replay_buffer import ReplayBuffer\n",
        "\n",
        "exp_replay = ReplayBuffer(10)\n",
        "\n",
        "for _ in range(30):\n",
        "    exp_replay.add(env.reset()[0], env.action_space.sample(), 1.0, env.reset()[0], done=False)\n",
        "\n",
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(5)\n",
        "\n",
        "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RnFX5sfZdC_"
      },
      "outputs": [],
      "source": [
        "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
        "    \"\"\"\n",
        "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer.\n",
        "    Whenever game ends due to termination or truncation, add record with done=terminated and reset the game.\n",
        "    It is guaranteed that env has terminated=False when passed to this function.\n",
        "\n",
        "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
        "\n",
        "    :returns: return sum of rewards over time and the state in which the env stays\n",
        "    \"\"\"\n",
        "    s = initial_state\n",
        "    sum_rewards = 0\n",
        "\n",
        "    # Play the game for n_steps as per instructions above\n",
        "    <YOUR CODE>\n",
        "\n",
        "    return sum_rewards, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXXmFEKGZdC_"
      },
      "outputs": [],
      "source": [
        "# testing your code.\n",
        "exp_replay = ReplayBuffer(2000)\n",
        "\n",
        "state, _ = env.reset()\n",
        "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
        "\n",
        "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
        "# just make sure you know what your code does\n",
        "assert len(exp_replay) == 1000, \"play_and_record should have added exactly 1000 steps, \" \"but instead added %i\" % len(\n",
        "    exp_replay\n",
        ")\n",
        "is_dones = list(zip(*exp_replay._storage))[-1]\n",
        "\n",
        "assert 0 < np.mean(is_dones) < 0.1, (\n",
        "    \"Please make sure you restart the game whenever it is 'done' and \"\n",
        "    \"record the is_done correctly into the buffer. Got %f is_done rate over \"\n",
        "    \"%i steps. [If you think it's your tough luck, just re-run the test]\" % (np.mean(is_dones), len(exp_replay))\n",
        ")\n",
        "\n",
        "for _ in range(100):\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
        "    assert act_batch.shape == (10,), \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
        "    assert reward_batch.shape == (10,), \"rewards batch should have shape (10,) but is instead %s\" % str(\n",
        "        reward_batch.shape\n",
        "    )\n",
        "    assert is_done_batch.shape == (10,), \"is_done batch should have shape (10,) but is instead %s\" % str(\n",
        "        is_done_batch.shape\n",
        "    )\n",
        "    assert [int(i) in (0, 1) for i in is_dones], \"is_done should be strictly True or False\"\n",
        "    assert [0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n",
        "\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoVGsnHRZdC_"
      },
      "source": [
        "### Target networks\n",
        "\n",
        "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
        "\n",
        "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
        "\n",
        "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BLJCNiuZdC_"
      },
      "outputs": [],
      "source": [
        "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
        "# This is how you can load weights from agent into target network\n",
        "target_network.load_state_dict(agent.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_GGShX3ZdC_"
      },
      "source": [
        "### Learning with... Q-learning\n",
        "Here we write a function similar to `agent.update` from tabular q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hbg-xANZdC_"
      },
      "source": [
        "Compute Q-learning TD error:\n",
        "\n",
        "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
        "\n",
        "With Q-reference defined as\n",
        "\n",
        "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
        "\n",
        "Where\n",
        "* $Q_{target}(s',a')$ denotes Q-value of next state and next action predicted by __target_network__\n",
        "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
        "* $\\gamma$ is a discount factor defined two cells above.\n",
        "\n",
        "\n",
        "__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n",
        "\n",
        "__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https://rubberduckdebugging.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxrEOC7mZdC_"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
        "                    agent, target_network,\n",
        "                    gamma=0.99,\n",
        "                    check_shapes=False,\n",
        "                    device=device):\n",
        "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
        "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
        "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
        "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
        "    # shape: [batch_size, *state_shape]\n",
        "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
        "    is_done = torch.tensor(\n",
        "        is_done.astype('float32'),\n",
        "        device=device,\n",
        "        dtype=torch.float32,\n",
        "    )  # shape: [batch_size]\n",
        "    is_not_done = 1 - is_done\n",
        "\n",
        "    # get q-values for all actions in current states\n",
        "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
        "\n",
        "    # compute q-values for all actions in next states\n",
        "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
        "\n",
        "    # select q-values for chosen actions\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
        "\n",
        "    # compute V*(next_states) using predicted next q-values\n",
        "    next_state_values = <YOUR CODE>\n",
        "\n",
        "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
        "        \"must predict one value per state\"\n",
        "\n",
        "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
        "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
        "    # you can multiply next state values by is_not_done to achieve this.\n",
        "    target_qvalues_for_actions = <YOUR CODE>\n",
        "\n",
        "    # mean squared error loss to minimize\n",
        "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
        "\n",
        "    if check_shapes:\n",
        "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
        "            \"make sure you predicted q-values for all actions in next state\"\n",
        "        assert next_state_values.data.dim() == 1, \\\n",
        "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
        "            \"there's something wrong with target q-values, they must be a vector\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgZKcPPnZdC_"
      },
      "source": [
        "Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp8eREoDZdC_"
      },
      "outputs": [],
      "source": [
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "\n",
        "loss = compute_td_loss(\n",
        "    obs_batch,\n",
        "    act_batch,\n",
        "    reward_batch,\n",
        "    next_obs_batch,\n",
        "    is_done_batch,\n",
        "    agent,\n",
        "    target_network,\n",
        "    gamma=0.99,\n",
        "    check_shapes=True,\n",
        ")\n",
        "loss.backward()\n",
        "\n",
        "assert loss.requires_grad and tuple(loss.data.size()) == (), \"you must return scalar loss - mean over batch\"\n",
        "assert np.any(\n",
        "    next(agent.parameters()).grad.data.cpu().numpy() != 0\n",
        "), \"loss must be differentiable w.r.t. network weights\"\n",
        "assert np.all(next(target_network.parameters()).grad is None), \"target network should not have grads\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A1QtGVqZdC_"
      },
      "source": [
        "### Main loop\n",
        "\n",
        "It's time to put everything together and see if it learns anything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lAUT94JZdC_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOk81bdZZdC_"
      },
      "outputs": [],
      "source": [
        "seed = <YOUR CODE: your favourite random seed>\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13K5t2CTZdDA"
      },
      "outputs": [],
      "source": [
        "state_dim = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "state, _ = env.reset(seed=seed)\n",
        "\n",
        "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
        "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
        "target_network.load_state_dict(agent.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD7PAlwQZdDA"
      },
      "outputs": [],
      "source": [
        "REPLAY_BUFFER_SIZE = 10**4\n",
        "\n",
        "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
        "for i in range(100):\n",
        "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
        "        print(\n",
        "            \"\"\"\n",
        "            Less than 100 Mb RAM available.\n",
        "            Make sure the buffer size in not too huge.\n",
        "            Also check, maybe other processes consume RAM heavily.\n",
        "            \"\"\"\n",
        "        )\n",
        "        break\n",
        "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
        "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
        "        break\n",
        "print(len(exp_replay))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl2VCEYQZdDA"
      },
      "outputs": [],
      "source": [
        "# # for something more complicated than CartPole\n",
        "\n",
        "# timesteps_per_epoch = 1\n",
        "# batch_size = 32\n",
        "# total_steps = 3 * 10**6\n",
        "# decay_steps = 1 * 10**6\n",
        "\n",
        "# opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "# init_epsilon = 1\n",
        "# final_epsilon = 0.1\n",
        "\n",
        "# loss_freq = 20\n",
        "# refresh_target_network_freq = 1000\n",
        "# eval_freq = 5000\n",
        "\n",
        "# max_grad_norm = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-sD-QyUZdDA"
      },
      "outputs": [],
      "source": [
        "timesteps_per_epoch = 1\n",
        "batch_size = 32\n",
        "total_steps = 4 * 10**4\n",
        "decay_steps = 1 * 10**4\n",
        "\n",
        "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "init_epsilon = 1\n",
        "final_epsilon = 0.1\n",
        "\n",
        "loss_freq = 20\n",
        "refresh_target_network_freq = 100\n",
        "eval_freq = 1000\n",
        "\n",
        "max_grad_norm = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piqDfKQAZdDA"
      },
      "outputs": [],
      "source": [
        "mean_rw_history = []\n",
        "td_loss_history = []\n",
        "grad_norm_history = []\n",
        "initial_state_v_history = []\n",
        "step = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks8NAV8AZdDA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "def wait_for_keyboard_interrupt():\n",
        "    try:\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    except KeyboardInterrupt:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU3GSGZqZdDA"
      },
      "outputs": [],
      "source": [
        "state, _ = env.reset()\n",
        "with trange(step, total_steps + 1) as progress_bar:\n",
        "    for step in progress_bar:\n",
        "        if not utils.is_enough_ram():\n",
        "            print('less that 100 Mb RAM available, freezing')\n",
        "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
        "            wait_for_keyboard_interrupt()\n",
        "\n",
        "        agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
        "\n",
        "        # play\n",
        "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
        "\n",
        "        # train\n",
        "        <YOUR CODE: sample batch_size of data from experience replay>\n",
        "\n",
        "        loss = <YOUR CODE: compute TD loss>\n",
        "\n",
        "        loss.backward()\n",
        "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if step % loss_freq == 0:\n",
        "            td_loss_history.append(loss.data.cpu().item())\n",
        "            grad_norm_history.append(grad_norm)\n",
        "\n",
        "        if step % refresh_target_network_freq == 0:\n",
        "            # Load agent weights into target_network\n",
        "            <YOUR CODE>\n",
        "\n",
        "        if step % eval_freq == 0:\n",
        "            mean_rw_history.append(evaluate(\n",
        "                make_env(), agent, n_games=3, greedy=True, t_max=1000, seed=step)\n",
        "            )\n",
        "            initial_state_q_values = agent.get_qvalues(\n",
        "                [make_env().reset(seed=step)[0]]\n",
        "            )\n",
        "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
        "\n",
        "            clear_output(True)\n",
        "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
        "                (len(exp_replay), agent.epsilon))\n",
        "\n",
        "            plt.figure(figsize=[16, 9])\n",
        "\n",
        "            plt.subplot(2, 2, 1)\n",
        "            plt.title(\"Mean reward per episode\")\n",
        "            plt.plot(mean_rw_history)\n",
        "            plt.grid()\n",
        "\n",
        "            assert not np.isnan(td_loss_history[-1])\n",
        "            plt.subplot(2, 2, 2)\n",
        "            plt.title(\"TD loss history (smoothened)\")\n",
        "            plt.plot(utils.smoothen(td_loss_history))\n",
        "            plt.grid()\n",
        "\n",
        "            plt.subplot(2, 2, 3)\n",
        "            plt.title(\"Initial state V\")\n",
        "            plt.plot(initial_state_v_history)\n",
        "            plt.grid()\n",
        "\n",
        "            plt.subplot(2, 2, 4)\n",
        "            plt.title(\"Grad norm history (smoothened)\")\n",
        "            plt.plot(utils.smoothen(grad_norm_history))\n",
        "            plt.grid()\n",
        "\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwWFT2SBZdDA"
      },
      "outputs": [],
      "source": [
        "final_score = evaluate(make_env(), agent, n_games=30, greedy=True, t_max=1000)\n",
        "print(\"final score:\", final_score)\n",
        "assert final_score > 300, \"not good enough for DQN\"\n",
        "print(\"Well done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-feeX9YZdDA"
      },
      "source": [
        "**Agent's predicted V-values vs their Monte-Carlo estimates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjVuSIrPZdDA"
      },
      "outputs": [],
      "source": [
        "eval_env = make_env()\n",
        "record = utils.play_and_log_episode(eval_env, agent)\n",
        "print(\"total reward for life:\", np.sum(record[\"rewards\"]))\n",
        "for key in record:\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCacwLw6ZdDA"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "ax.scatter(record[\"v_mc\"], record[\"v_agent\"])\n",
        "ax.plot(sorted(record[\"v_mc\"]), sorted(record[\"v_mc\"]), \"black\", linestyle=\"--\", label=\"x=y\")\n",
        "\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "ax.set_title(\"State Value Estimates\")\n",
        "ax.set_xlabel(\"Monte-Carlo\")\n",
        "ax.set_ylabel(\"Agent\")\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}